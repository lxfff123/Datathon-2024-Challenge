{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss7CKuDuefEc",
        "outputId": "e4736594-678b-404d-86d5-ebfd4b3c0b5c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.2.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import roc_auc_score, make_scorer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Import models for comparison\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier,\n",
        "    HistGradientBoostingClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Load data\n",
        "def load_data(train_path, test_path):\n",
        "    train = pd.read_csv(train_path)\n",
        "    test = pd.read_csv(test_path)\n",
        "    return train, test\n",
        "\n",
        "# Separate features and target variable\n",
        "def preprocess_data(train, test, target_column='DiagPeriodL90D'):\n",
        "    X = train.drop(columns=[target_column, 'patient_id'])\n",
        "    y = train[target_column]\n",
        "    X_test = test.drop(columns=['patient_id'])\n",
        "    return X, y, X_test\n",
        "\n",
        "# Define preprocessors\n",
        "def create_preprocessor(X):\n",
        "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ]\n",
        "    )\n",
        "    return preprocessor\n",
        "\n",
        "# Initialize models\n",
        "def initialize_models():\n",
        "    return {\n",
        "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
        "        'CatBoost': CatBoostClassifier(verbose=0, random_state=42),\n",
        "        'LightGBM': LGBMClassifier(random_state=42),\n",
        "        'SVC': SVC(probability=True, random_state=42),\n",
        "        'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "        'GaussianNB': GaussianNB(),\n",
        "        'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=42),\n",
        "        'HistGradientBoosting': HistGradientBoostingClassifier(random_state=42),\n",
        "        'Voting Classifier': VotingClassifier(estimators=[\n",
        "            ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "            ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
        "            ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)),\n",
        "            ('cat', CatBoostClassifier(verbose=0, random_state=42)),\n",
        "            ('lgb', LGBMClassifier(random_state=42))\n",
        "        ], voting='soft')\n",
        "    }\n",
        "\n",
        "# Evaluate models\n",
        "def evaluate_models(models, preprocessor, X, y, n_splits=5):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
        "    results = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])\n",
        "        cv_scores = cross_val_score(pipeline, X, y, cv=skf, scoring=scorer, n_jobs=-1)\n",
        "        results[name] = {\n",
        "            'Mean ROC-AUC': cv_scores.mean(),\n",
        "            'Std Dev': cv_scores.std(),\n",
        "            'Scores': cv_scores\n",
        "        }\n",
        "        print(f\"{name}: Mean ROC-AUC = {cv_scores.mean():.4f}, Std = {cv_scores.std():.4f}\")\n",
        "    return pd.DataFrame(results).T\n",
        "\n",
        "# Train best model on full data and make predictions\n",
        "def train_best_model_and_predict(best_model_name, models, preprocessor, X, y, X_test, test_data):\n",
        "    best_model = models[best_model_name]\n",
        "    final_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', best_model)])\n",
        "    final_pipeline.fit(X, y)\n",
        "\n",
        "    predictions = final_pipeline.predict_proba(X_test)[:, 1]  # Probability of DiagPeriodL90D=1\n",
        "    submission = pd.DataFrame({\n",
        "        'patient_id': test_data['patient_id'],\n",
        "        'DiagPeriodL90D': predictions\n",
        "    })\n",
        "    submission.to_csv('submission.csv', index=False)\n",
        "    print(f\"Submission file created as 'submission.csv' with best model: {best_model_name}\")"
      ],
      "metadata": {
        "id": "KIQtyV4NeZdO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f7edab7-4679-4a11-ffe7-d75808fc5d54"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier  # Ensuring this is imported if used\n",
        "\n",
        "# Custom transformer to convert sparse matrix to dense matrix\n",
        "class SparseToDenseTransformer(TransformerMixin, BaseEstimator):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return X.toarray() if hasattr(X, 'toarray') else X\n",
        "\n",
        "# Function to check if model needs dense data\n",
        "def needs_dense_data(model):\n",
        "    # Add models that require dense data here\n",
        "    return isinstance(model, (GaussianNB, HistGradientBoostingClassifier))\n",
        "\n",
        "# Adjusted evaluate_models function\n",
        "def evaluate_models(models, preprocessor, X, y, n_splits=5):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
        "    results = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        if needs_dense_data(model):\n",
        "            # Wrap models needing dense data with SparseToDenseTransformer\n",
        "            pipeline = Pipeline(steps=[\n",
        "                ('preprocessor', preprocessor),\n",
        "                ('to_dense', SparseToDenseTransformer()),\n",
        "                ('classifier', model)\n",
        "            ])\n",
        "        else:\n",
        "            # Standard pipeline without conversion for other models\n",
        "            pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])\n",
        "\n",
        "        # Evaluate using cross-validation\n",
        "        cv_scores = cross_val_score(pipeline, X, y, cv=skf, scoring=scorer, n_jobs=-1)\n",
        "\n",
        "        results[name] = {\n",
        "            'Mean ROC-AUC': cv_scores.mean(),\n",
        "            'Std Dev': cv_scores.std(),\n",
        "            'Scores': cv_scores\n",
        "        }\n",
        "        print(f\"{name}: Mean ROC-AUC = {cv_scores.mean():.4f}, Std = {cv_scores.std():.4f}\")\n",
        "\n",
        "    return pd.DataFrame(results).T"
      ],
      "metadata": {
        "id": "LAs_lnbCgaDY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Paths to data files\n",
        "    train_path = 'processed_train_data.csv'\n",
        "    test_path = 'processed_test_data.csv'\n",
        "\n",
        "    # Load datasets\n",
        "    train_data, test_data = load_data(train_path, test_path)\n",
        "\n",
        "    # Preprocess data\n",
        "    X, y, X_test = preprocess_data(train_data, test_data)\n",
        "\n",
        "    # Create preprocessor\n",
        "    preprocessor = create_preprocessor(X)\n",
        "\n",
        "    # Initialize models\n",
        "    models = initialize_models()\n",
        "\n",
        "    # Evaluate models and get results\n",
        "    model_results = evaluate_models(models, preprocessor, X, y)\n",
        "\n",
        "    # Select the best model by highest ROC-AUC mean\n",
        "    best_model_name = model_results['Mean ROC-AUC'].idxmax()\n",
        "    print(f\"\\nBest model: {best_model_name} with Mean ROC-AUC: {model_results['Mean ROC-AUC'].max():.4f}\")\n",
        "\n",
        "    # Train the best model and predict on test set\n",
        "    train_best_model_and_predict(best_model_name, models, preprocessor, X, y, X_test, test_data)\n"
      ],
      "metadata": {
        "id": "jZQ7z7BXgfeI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "47646dd7-639d-4a46-f39e-387b9f8dc092"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ade4420cee89>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Evaluate models and get results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmodel_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Select the best model by highest ROC-AUC mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-74c63a18c349>\u001b[0m in \u001b[0;36mevaluate_models\u001b[0;34m(models, preprocessor, X, y, n_splits)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Evaluate using cross-validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mcv_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         results[name] = {\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    424\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    425\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid for Gradient Boosting\n",
        "param_grid = {\n",
        "    'classifier__min_samples_split': [2, 5, 10],\n",
        "    'classifier__max_depth': [3, 5, 7],\n",
        "    'classifier__learning_rate': [0.01, 0.1, 1.0]\n",
        "}\n",
        "\n",
        "# Create a GradientBoostingClassifier instance\n",
        "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Create a pipeline with the preprocessor and GradientBoostingClassifier\n",
        "gb_pipeline = Pipeline([('preprocessor', preprocessor), ('classifier', gb_model)])\n",
        "\n",
        "# Perform GridSearchCV to find the best hyperparameters\n",
        "grid_search = GridSearchCV(gb_pipeline, param_grid, scoring='roc_auc', cv=5, n_jobs=-1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the results for each hyperparameter combination\n",
        "print(\"Gradient Boosting Hyperparameter Tuning Results:\")\n",
        "results = pd.DataFrame(grid_search.cv_results_)\n",
        "print(results[['params', 'mean_test_score', 'std_test_score']])\n",
        "\n",
        "\n",
        "# Get the best hyperparameters and score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(f\"\\nBest Hyperparameters: {best_params}\")\n",
        "print(f\"Best Mean ROC-AUC Score: {best_score:.4f}\")"
      ],
      "metadata": {
        "id": "3w7PvhFZklfh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d64fd8fd-6698-4d8b-d245-515af1490566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Hyperparameter Tuning Results:\n",
            "                                               params  mean_test_score  \\\n",
            "0   {'classifier__learning_rate': 0.01, 'classifie...         0.758230   \n",
            "1   {'classifier__learning_rate': 0.01, 'classifie...         0.758230   \n",
            "2   {'classifier__learning_rate': 0.01, 'classifie...         0.758236   \n",
            "3   {'classifier__learning_rate': 0.01, 'classifie...         0.768243   \n",
            "4   {'classifier__learning_rate': 0.01, 'classifie...         0.768254   \n",
            "5   {'classifier__learning_rate': 0.01, 'classifie...         0.768250   \n",
            "6   {'classifier__learning_rate': 0.01, 'classifie...         0.786525   \n",
            "7   {'classifier__learning_rate': 0.01, 'classifie...         0.786712   \n",
            "8   {'classifier__learning_rate': 0.01, 'classifie...         0.787032   \n",
            "9   {'classifier__learning_rate': 0.1, 'classifier...         0.799470   \n",
            "10  {'classifier__learning_rate': 0.1, 'classifier...         0.799276   \n",
            "11  {'classifier__learning_rate': 0.1, 'classifier...         0.799058   \n",
            "12  {'classifier__learning_rate': 0.1, 'classifier...         0.798345   \n",
            "13  {'classifier__learning_rate': 0.1, 'classifier...         0.799395   \n",
            "14  {'classifier__learning_rate': 0.1, 'classifier...         0.801476   \n",
            "15  {'classifier__learning_rate': 0.1, 'classifier...         0.797145   \n",
            "16  {'classifier__learning_rate': 0.1, 'classifier...         0.799218   \n",
            "17  {'classifier__learning_rate': 0.1, 'classifier...         0.798371   \n",
            "18  {'classifier__learning_rate': 1.0, 'classifier...         0.770001   \n",
            "19  {'classifier__learning_rate': 1.0, 'classifier...         0.775161   \n",
            "20  {'classifier__learning_rate': 1.0, 'classifier...         0.779448   \n",
            "21  {'classifier__learning_rate': 1.0, 'classifier...         0.750503   \n",
            "22  {'classifier__learning_rate': 1.0, 'classifier...         0.755217   \n",
            "23  {'classifier__learning_rate': 1.0, 'classifier...         0.754270   \n",
            "24  {'classifier__learning_rate': 1.0, 'classifier...         0.733768   \n",
            "25  {'classifier__learning_rate': 1.0, 'classifier...         0.741279   \n",
            "26  {'classifier__learning_rate': 1.0, 'classifier...         0.746848   \n",
            "\n",
            "    std_test_score  \n",
            "0         0.010036  \n",
            "1         0.010036  \n",
            "2         0.010031  \n",
            "3         0.008433  \n",
            "4         0.008477  \n",
            "5         0.008480  \n",
            "6         0.011836  \n",
            "7         0.012359  \n",
            "8         0.012210  \n",
            "9         0.009234  \n",
            "10        0.008274  \n",
            "11        0.008763  \n",
            "12        0.008799  \n",
            "13        0.010218  \n",
            "14        0.009062  \n",
            "15        0.009968  \n",
            "16        0.009188  \n",
            "17        0.009845  \n",
            "18        0.008748  \n",
            "19        0.006788  \n",
            "20        0.003963  \n",
            "21        0.006930  \n",
            "22        0.012220  \n",
            "23        0.007614  \n",
            "24        0.011702  \n",
            "25        0.013629  \n",
            "26        0.011413  \n",
            "\n",
            "Best Hyperparameters: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 5, 'classifier__min_samples_split': 10}\n",
            "Best Mean ROC-AUC Score: 0.8015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the parameter grid for CatBoost\n",
        "param_grid_catboost = {\n",
        "    'classifier__iterations': [100, 200],\n",
        "    'classifier__learning_rate': [0.01, 0.1],\n",
        "    'classifier__depth': [4, 6],\n",
        "    'classifier__random_strength': [0.5, 1.0],  # Added random_strength\n",
        "    'classifier__bagging_temperature': [0.0, 0.5]  # Added bagging_temperature\n",
        "}\n",
        "\n",
        "# Create a CatBoostClassifier instance\n",
        "catboost_model = CatBoostClassifier(verbose=0, random_state=42)\n",
        "\n",
        "# Create a pipeline with the preprocessor and CatBoostClassifier\n",
        "catboost_pipeline = Pipeline([('preprocessor', preprocessor), ('classifier', catboost_model)])\n",
        "\n",
        "# Perform GridSearchCV to find the best hyperparameters\n",
        "grid_search_catboost = GridSearchCV(catboost_pipeline, param_grid_catboost, scoring='roc_auc', cv=5, n_jobs=-1)\n",
        "grid_search_catboost.fit(X, y)\n",
        "\n",
        "# Print the results for each hyperparameter combination\n",
        "print(\"\\nCatBoost Hyperparameter Tuning Results:\")\n",
        "results_catboost = pd.DataFrame(grid_search_catboost.cv_results_)\n",
        "print(results_catboost[['params', 'mean_test_score', 'std_test_score']])\n",
        "\n",
        "# Get the best hyperparameters and score for CatBoost\n",
        "best_params_catboost = grid_search_catboost.best_params_\n",
        "best_score_catboost = grid_search_catboost.best_score_\n",
        "\n",
        "print(f\"\\nBest Hyperparameters for CatBoost: {best_params_catboost}\")\n",
        "print(f\"Best Mean ROC-AUC Score for CatBoost: {best_score_catboost:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvOuEaBO60tC",
        "outputId": "c0301c3d-eed5-466a-a531-7119609aea62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CatBoost Hyperparameter Tuning Results:\n",
            "                                               params  mean_test_score  \\\n",
            "0   {'classifier__bagging_temperature': 0.0, 'clas...         0.788666   \n",
            "1   {'classifier__bagging_temperature': 0.0, 'clas...         0.789551   \n",
            "2   {'classifier__bagging_temperature': 0.0, 'clas...         0.802563   \n",
            "3   {'classifier__bagging_temperature': 0.0, 'clas...         0.801909   \n",
            "4   {'classifier__bagging_temperature': 0.0, 'clas...         0.794577   \n",
            "5   {'classifier__bagging_temperature': 0.0, 'clas...         0.793403   \n",
            "6   {'classifier__bagging_temperature': 0.0, 'clas...         0.803177   \n",
            "7   {'classifier__bagging_temperature': 0.0, 'clas...         0.804307   \n",
            "8   {'classifier__bagging_temperature': 0.0, 'clas...         0.793626   \n",
            "9   {'classifier__bagging_temperature': 0.0, 'clas...         0.792958   \n",
            "10  {'classifier__bagging_temperature': 0.0, 'clas...         0.804366   \n",
            "11  {'classifier__bagging_temperature': 0.0, 'clas...         0.802596   \n",
            "12  {'classifier__bagging_temperature': 0.0, 'clas...         0.798610   \n",
            "13  {'classifier__bagging_temperature': 0.0, 'clas...         0.796256   \n",
            "14  {'classifier__bagging_temperature': 0.0, 'clas...         0.802777   \n",
            "15  {'classifier__bagging_temperature': 0.0, 'clas...         0.803526   \n",
            "16  {'classifier__bagging_temperature': 0.5, 'clas...         0.788666   \n",
            "17  {'classifier__bagging_temperature': 0.5, 'clas...         0.789551   \n",
            "18  {'classifier__bagging_temperature': 0.5, 'clas...         0.802563   \n",
            "19  {'classifier__bagging_temperature': 0.5, 'clas...         0.801909   \n",
            "20  {'classifier__bagging_temperature': 0.5, 'clas...         0.794577   \n",
            "21  {'classifier__bagging_temperature': 0.5, 'clas...         0.793403   \n",
            "22  {'classifier__bagging_temperature': 0.5, 'clas...         0.803177   \n",
            "23  {'classifier__bagging_temperature': 0.5, 'clas...         0.804307   \n",
            "24  {'classifier__bagging_temperature': 0.5, 'clas...         0.793626   \n",
            "25  {'classifier__bagging_temperature': 0.5, 'clas...         0.792958   \n",
            "26  {'classifier__bagging_temperature': 0.5, 'clas...         0.804366   \n",
            "27  {'classifier__bagging_temperature': 0.5, 'clas...         0.802596   \n",
            "28  {'classifier__bagging_temperature': 0.5, 'clas...         0.798610   \n",
            "29  {'classifier__bagging_temperature': 0.5, 'clas...         0.796256   \n",
            "30  {'classifier__bagging_temperature': 0.5, 'clas...         0.802777   \n",
            "31  {'classifier__bagging_temperature': 0.5, 'clas...         0.803526   \n",
            "\n",
            "    std_test_score  \n",
            "0         0.011007  \n",
            "1         0.010376  \n",
            "2         0.008753  \n",
            "3         0.010484  \n",
            "4         0.010689  \n",
            "5         0.011547  \n",
            "6         0.010030  \n",
            "7         0.010723  \n",
            "8         0.010415  \n",
            "9         0.011194  \n",
            "10        0.008697  \n",
            "11        0.010769  \n",
            "12        0.012000  \n",
            "13        0.010807  \n",
            "14        0.007980  \n",
            "15        0.010742  \n",
            "16        0.011007  \n",
            "17        0.010376  \n",
            "18        0.008753  \n",
            "19        0.010484  \n",
            "20        0.010689  \n",
            "21        0.011547  \n",
            "22        0.010030  \n",
            "23        0.010723  \n",
            "24        0.010415  \n",
            "25        0.011194  \n",
            "26        0.008697  \n",
            "27        0.010769  \n",
            "28        0.012000  \n",
            "29        0.010807  \n",
            "30        0.007980  \n",
            "31        0.010742  \n",
            "\n",
            "Best Hyperparameters for CatBoost: {'classifier__bagging_temperature': 0.0, 'classifier__depth': 6, 'classifier__iterations': 100, 'classifier__learning_rate': 0.1, 'classifier__random_strength': 0.5}\n",
            "Best Mean ROC-AUC Score for CatBoost: 0.8044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a CatBoostClassifier with the best hyperparameters from the provided example\n",
        "catboost_model = CatBoostClassifier(\n",
        "    bagging_temperature=0.0,\n",
        "    depth=6,\n",
        "    iterations=100,\n",
        "    learning_rate=0.1,\n",
        "    random_strength=0.5,\n",
        "    verbose=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create the pipeline\n",
        "catboost_pipeline = Pipeline([('preprocessor', preprocessor), ('classifier', catboost_model)])\n",
        "\n",
        "# Train the CatBoost model using the best hyperparameters\n",
        "catboost_pipeline.fit(X, y)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = catboost_pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "binary_predictions = [1.0 if prob >= 0.5 else 0.0 for prob in predictions]  # Threshold at 0.5\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'patient_id': test_data['patient_id'],\n",
        "    'DiagPeriodL90D': binary_predictions\n",
        "})\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "submission.to_csv('catboost_submission.csv', index=False)\n",
        "print(\"CatBoost predictions saved to catboost_submission.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0-_x-Rns0em",
        "outputId": "1679a008-78be-4f03-e28e-11547f642674"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost predictions saved to catboost_submission.csv\n"
          ]
        }
      ]
    }
  ]
}